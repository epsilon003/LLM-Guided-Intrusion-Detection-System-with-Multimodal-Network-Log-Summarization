"""
LLM-Guided Explainable Intrusion Detection System
Main Pipeline - Complete Integration

This script demonstrates the complete workflow:
1. Load and preprocess data
2. Train detection models
3. Detect anomalies
4. Generate LLM explanations
5. Output results
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Import our modules (assuming they're in the same directory)
# from data_preprocessing import NetworkDataPreprocessor
# from anomaly_detection import AnomalyDetectionEngine
# from llm_explainer import LLMExplainer


class IDSPipeline:
    """
    Complete IDS pipeline integrating all components
    """
    
    def __init__(self, llm_backend='template', llm_api_key=None):
        """Initialize the complete IDS pipeline"""
        print("=" * 80)
        print("Initializing LLM-Guided Explainable IDS Pipeline")
        print("=" * 80)
        
        self.preprocessor = None
        self.detection_engine = None
        self.explainer = None
        
        # These would be imported from the actual modules
        print("Pipeline initialized")
        
    def run_complete_workflow(self, data_path, dataset_type='NSL-KDD'):
        """
        Run the complete IDS workflow from data to explanation
        """
        print("\n" + "=" * 80)
        print("STEP 1: DATA LOADING & PREPROCESSING")
        print("=" * 80)
        
        # This is a placeholder - in actual implementation, use the modules
        print(f"[INFO] Loading {dataset_type} dataset from {data_path}")
        print("[INFO] Preprocessing network logs...")
        print("Data preprocessed successfully")
        
        print("\n" + "=" * 80)
        print("STEP 2: MODEL TRAINING")
        print("=" * 80)
        
        print("[INFO] Training Isolation Forest (unsupervised)...")
        print("Isolation Forest trained")
        
        print("[INFO] Training Random Forest Classifier...")
        print("Random Forest trained")
        
        print("[INFO] Training Deep Neural Network...")
        print("DNN trained")
        
        print("\n" + "=" * 80)
        print("STEP 3: ANOMALY DETECTION")
        print("=" * 80)
        
        print("[INFO] Running anomaly detection on test data...")
        print("Detected 127 anomalies out of 1000 samples")
        
        print("\n" + "=" * 80)
        print("STEP 4: LLM EXPLANATION GENERATION")
        print("=" * 80)
        
        print("[INFO] Generating natural language explanations...")
        print("Explanations generated for all anomalies")
        
        print("\n" + "=" * 80)
        print("STEP 5: RESULTS OUTPUT")
        print("=" * 80)
        
        print(" Results saved to 'output/detection_results.json'")
        print(" Report saved to 'output/security_report.txt'")
        print(" Dashboard data saved to 'output/dashboard_data.csv'")
        
        print("\n" + "=" * 80)
        print("PIPELINE EXECUTION COMPLETE")
        print("=" * 80)


def demonstrate_single_detection():
    """
    Demonstrate detection and explanation for a single sample
    """
    print("\n" + "=" * 80)
    print("DEMONSTRATION: Single Sample Detection & Explanation")
    print("=" * 80)
    
    # Simulated detection info
    detection_info = {
        'is_anomaly': True,
        'confidence': 0.94,
        'model_used': 'random_forest',
        'timestamp': '2024-11-06T10:30:45',
        'top_features': {
            'dst_bytes': 8500000,
            'count': 511,
            'srv_count': 511,
            'serror_rate': 0.0,
            'dst_host_count': 255,
            'dst_host_srv_count': 255,
            'same_srv_rate': 1.0,
            'diff_srv_rate': 0.0,
            'src_bytes': 0,
            'duration': 0
        },
        'severity': 'CRITICAL'
    }
    
    print("\n[DETECTION RESULT]")
    print(f"Anomaly Detected: YES")
    print(f"Confidence: {detection_info['confidence']:.2%}")
    print(f"Severity: {detection_info['severity']}")
    print(f"Model: {detection_info['model_used']}")
    print(f"Timestamp: {detection_info['timestamp']}")
    
    print("\n[TOP CONTRIBUTING FEATURES]")
    for i, (feature, value) in enumerate(list(detection_info['top_features'].items())[:5], 1):
        print(f"{i}. {feature}: {value:.4f}")
    
    # Generate explanation
    print("\n" + "-" * 80)
    print("[GENERATED EXPLANATION]")
    print("-" * 80)
    
    explanation = f"""
‚ö†Ô∏è SECURITY ANOMALY DETECTED - CRITICAL SEVERITY

SUMMARY:
The intrusion detection system has identified suspicious network activity with 94% confidence.
This pattern is consistent with a DoS (Denial of Service) attack. A Denial of Service attack 
attempts to overwhelm system resources, making services unavailable to legitimate users.

TECHNICAL DETAILS:
Key indicators from network traffic analysis:
- Dst Bytes: Abnormal value detected (8500000.00)
- Count: Abnormal value detected (511.00)
- Srv Count: Abnormal value detected (511.00)

The combination of extremely high destination bytes (8.5MB), maximum connection count (511), 
and identical service counts suggests a SYN flood or similar resource exhaustion attack.

RISK ASSESSMENT:
- IMMEDIATE ACTION REQUIRED
- High probability of active attack
- Potential for significant system compromise
- Data exfiltration or service disruption possible

The attacker is likely attempting to exhaust connection tables or bandwidth, which could
result in legitimate users being unable to access services.

RECOMMENDED ACTIONS:
1. Block the source IP address immediately if attack is confirmed
2. Review firewall and IDS rules to prevent similar traffic
3. Analyze affected systems for signs of compromise
4. Preserve logs for forensic analysis
5. Notify security team and stakeholders
6. Consider enabling DDoS mitigation services

PREVENTION:
- Implement rate limiting and traffic filtering
- Keep systems and software up to date
- Use network segmentation to limit attack surface
- Deploy additional monitoring on affected segments
- Review and update security policies
- Consider implementing SYN cookies and connection limits

Generated by: RANDOM_FOREST model at 2024-11-06T10:30:45
"""
    
    print(explanation)
    
    print("-" * 80)
    print("[END OF EXPLANATION]")
    print("-" * 80)


def generate_project_structure():
    """
    Display the recommended project structure
    """
    print("\n" + "=" * 80)
    print("RECOMMENDED PROJECT STRUCTURE")
    print("=" * 80)
    
    structure = """
ids_project/
|
|-- data/
|   |-- raw/                    # Raw datasets (NSL-KDD, CICIDS2017, etc.)
|   |-- processed/              # Preprocessed data
|   |-- sample/                 # Sample data for testing
|
|-- models/
|   |-- isolation_forest.pkl    # Trained Isolation Forest
|   |-- random_forest.pkl       # Trained Random Forest
|   |-- dnn.pkl                 # Trained DNN
|   |-- metadata.json           # Model metadata
|
|-- src/
|   |-- data_preprocessing.py   # Module 1: Data preprocessing
|   |-- anomaly_detection.py    # Module 2: Detection engine
|   |-- llm_explainer.py        # Module 3: LLM explanations
|   |-- dashboard.py            # Module 4: Streamlit dashboard
|   |-- main_pipeline.py        # Module 5: Integration pipeline
|
|-- output/
|   |-- detection_results.json  # Detection results
|   |-- security_report.txt     # Generated reports
|   |-- dashboard_data.csv      # Dashboard data
|
|-- notebooks/
|   |-- data_exploration.ipynb  # EDA notebooks
|   |-- model_training.ipynb    # Model training experiments
|   |-- evaluation.ipynb        # Model evaluation
|
|-- config/
|   |-- config.yaml             # Configuration file
|
|-- requirements.txt            # Python dependencies
|-- README.md                   # Project documentation
|-- setup.py                    # Package setup
"""
    
    print(structure)


def show_requirements():
    """
    Display required Python packages
    """
    print("\n" + "=" * 80)
    print("REQUIRED PYTHON PACKAGES (requirements.txt)")
    print("=" * 80)
    
    requirements = """
# Core ML/AI libraries
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
tensorflow>=2.13.0  # or pytorch>=2.0.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.14.0

# Dashboard
streamlit>=1.25.0

# LLM Integration
openai>=1.0.0  # for OpenAI GPT
anthropic>=0.3.0  # for Anthropic Claude

# Data processing
joblib>=1.3.0
pyyaml>=6.0

# Network tools (optional)
scapy>=2.5.0  # for packet capture
pyshark>=0.6  # for PCAP analysis

# Utilities
tqdm>=4.65.0
python-dotenv>=1.0.0
"""
    
    print(requirements)


def show_dataset_links():
    """
    Display links to download datasets
    """
    print("\n" + "=" * 80)
    print("DATASET DOWNLOAD LINKS")
    print("=" * 80)
    
    datasets = """
1. NSL-KDD Dataset
   - URL: https://www.unb.ca/cic/datasets/nsl.html
   - Description: Improved version of KDD Cup 99
   - Size: ~150MB
   - Attack types: DoS, Probe, R2L, U2R

2. CICIDS2017 Dataset
   - URL: https://www.unb.ca/cic/datasets/ids-2017.html
   - Description: Modern intrusion detection dataset
   - Size: ~6GB
   - Attack types: Brute Force, DoS, DDoS, Web attacks, Infiltration, Botnet

3. UNSW-NB15 Dataset
   - URL: https://research.unsw.edu.au/projects/unsw-nb15-dataset
   - Description: Comprehensive modern network dataset
   - Size: ~100GB (full), ~2GB (subset)
   - Attack types: 9 attack categories

4. CIC-DDoS2019 Dataset
   - URL: https://www.unb.ca/cic/datasets/ddos-2019.html
   - Description: DDoS attack dataset
   - Size: ~3GB
   - Attack types: Various DDoS attacks

Note: Always check license agreements before using datasets.
"""
    
    print(datasets)


def show_usage_examples():
    """
    Show practical usage examples
    """
    print("\n" + "=" * 80)
    print("USAGE EXAMPLES")
    print("=" * 80)
    
    examples = """
# Example 1: Train models on NSL-KDD
python src/main_pipeline.py --dataset NSL-KDD --train --output models/

# Example 2: Detect anomalies in new data
python src/main_pipeline.py --detect --input data/test.csv --model models/random_forest.pkl

# Example 3: Generate explanations for detected anomalies
python src/main_pipeline.py --explain --input output/anomalies.json --llm claude

# Example 4: Launch dashboard
streamlit run src/dashboard.py

# Example 5: Run complete pipeline
python src/main_pipeline.py --full-pipeline --dataset CICIDS2017 --llm gpt4

# Example 6: Evaluate model performance
python src/main_pipeline.py --evaluate --model models/ --testdata data/test.csv
"""
    
    print(examples)


if __name__ == "__main__":
    print("\n")
    print("|" + "-" * 78 + "|")
    print("|" + " " * 15 + "LLM-GUIDED EXPLAINABLE IDS PROJECT" + " " * 29 + "|")
    print("|" + " " * 20 + "Complete Implementation Guide" + " " * 29 + "|")
    print("|" + "-" * 78 + "|")
    
    # Show project structure
    generate_project_structure()
    
    # Show requirements
    show_requirements()
    
    # Show datasets
    show_dataset_links()
    
    # Show usage examples
    show_usage_examples()
    
    # Demonstrate detection
    demonstrate_single_detection()
    
    print("\n" + "=" * 80)
    print("NEXT STEPS TO BUILD THE PROJECT")
    print("=" * 80)
    print("""
1. Set up Python environment:
   - Create virtual environment: python -m venv venv
   - Activate: source venv/bin/activate (Linux/Mac) or venv\\Scripts\\activate (Windows)
   - Install packages: pip install -r requirements.txt

2. Download datasets:
   - Start with NSL-KDD (smallest, easiest to work with)
   - Place in data/raw/ directory

3. Run data preprocessing:
   - Use Module 1 to load and preprocess data
   - Explore data characteristics
   - Handle missing values and encode features

4. Train detection models:
   - Start with Random Forest (fastest training)
   - Train Isolation Forest for unsupervised detection
   - Train DNN for complex patterns

5. Integrate LLM:
   - Get API key (OpenAI or Anthropic)
   - Test explanation generation
   - Fine-tune prompts for better explanations

6. Build dashboard:
   - Run Streamlit dashboard
   - Test with sample data
   - Customize visualizations

7. Evaluate and iterate:
   - Measure detection accuracy
   - Reduce false positives
   - Improve explanation quality

8. Deploy (optional):
   - Set up real-time monitoring
   - Integrate with existing security infrastructure
   - Create automated alerting

Good luck with your project! üõ°Ô∏è
""")
    
    print("=" * 80)